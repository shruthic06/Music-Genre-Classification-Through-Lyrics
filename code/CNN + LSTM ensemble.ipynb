{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39f01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saksh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 genres: ['Pop', 'Rock', 'Country', 'R&B', 'Folk']\n",
      "Encoded genres: ['Country' 'Folk' 'Pop' 'R&B' 'Rock']\n",
      "Train size: 3925, Test size: 982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 3925/3925 [11:18<00:00,  5.79it/s]\n",
      "Embedding: 100%|██████████| 982/982 [01:28<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3140, Val size: 785, Test size: 982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 785/785 [01:10<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN...\n",
      "Epoch 1: Train Loss = 1.5216, Val Loss = 1.2647\n",
      "Epoch 2: Train Loss = 1.2765, Val Loss = 1.0959\n",
      "Epoch 3: Train Loss = 1.1529, Val Loss = 0.9145\n",
      "Epoch 4: Train Loss = 1.0117, Val Loss = 0.8095\n",
      "Epoch 5: Train Loss = 0.8633, Val Loss = 0.6078\n",
      "Epoch 6: Train Loss = 0.7423, Val Loss = 0.5077\n",
      "Epoch 7: Train Loss = 0.6419, Val Loss = 0.3805\n",
      "Epoch 8: Train Loss = 0.5308, Val Loss = 0.2659\n",
      "Epoch 9: Train Loss = 0.4572, Val Loss = 0.2051\n",
      "Epoch 10: Train Loss = 0.3911, Val Loss = 0.2256\n",
      "Epoch 11: Train Loss = 0.3269, Val Loss = 0.1723\n",
      "Epoch 12: Train Loss = 0.2905, Val Loss = 0.1279\n",
      "Epoch 13: Train Loss = 0.2757, Val Loss = 0.0871\n",
      "Epoch 14: Train Loss = 0.2445, Val Loss = 0.0697\n",
      "Epoch 15: Train Loss = 0.2116, Val Loss = 0.0561\n",
      "Epoch 16: Train Loss = 0.1785, Val Loss = 0.0598\n",
      "Epoch 17: Train Loss = 0.1959, Val Loss = 0.0408\n",
      "Epoch 18: Train Loss = 0.1667, Val Loss = 0.0379\n",
      "Epoch 19: Train Loss = 0.1683, Val Loss = 0.0414\n",
      "Epoch 20: Train Loss = 0.1793, Val Loss = 0.0404\n",
      "Epoch 21: Train Loss = 0.1676, Val Loss = 0.0289\n",
      "Epoch 22: Train Loss = 0.1544, Val Loss = 0.0258\n",
      "Epoch 23: Train Loss = 0.1510, Val Loss = 0.0222\n",
      "Epoch 24: Train Loss = 0.1402, Val Loss = 0.0248\n",
      "Epoch 25: Train Loss = 0.1568, Val Loss = 0.0137\n",
      "\n",
      "CNN Evaluation:\n",
      "Accuracy: 0.6038696537678208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.69      0.67      0.68       200\n",
      "        Folk       0.55      0.65      0.59       182\n",
      "         Pop       0.58      0.58      0.58       200\n",
      "         R&B       0.67      0.63      0.65       200\n",
      "        Rock       0.54      0.50      0.52       200\n",
      "\n",
      "    accuracy                           0.60       982\n",
      "   macro avg       0.61      0.60      0.60       982\n",
      "weighted avg       0.61      0.60      0.60       982\n",
      "\n",
      "\n",
      "Training LSTM...\n",
      "Epoch 1: Train Loss = 1.5138, Val Loss = 1.4416\n",
      "Epoch 2: Train Loss = 1.3960, Val Loss = 1.3429\n",
      "Epoch 3: Train Loss = 1.3261, Val Loss = 1.3155\n",
      "Epoch 4: Train Loss = 1.2681, Val Loss = 1.1717\n",
      "Epoch 5: Train Loss = 1.2318, Val Loss = 1.1629\n",
      "Epoch 6: Train Loss = 1.1703, Val Loss = 1.0408\n",
      "Epoch 7: Train Loss = 1.0900, Val Loss = 0.9985\n",
      "Epoch 8: Train Loss = 1.0207, Val Loss = 0.8911\n",
      "Epoch 9: Train Loss = 0.9263, Val Loss = 0.8245\n",
      "Epoch 10: Train Loss = 0.8351, Val Loss = 0.7586\n",
      "Epoch 11: Train Loss = 0.7435, Val Loss = 0.6692\n",
      "Epoch 12: Train Loss = 0.6852, Val Loss = 0.5274\n",
      "Epoch 13: Train Loss = 0.5778, Val Loss = 0.4795\n",
      "Epoch 14: Train Loss = 0.5027, Val Loss = 0.2993\n",
      "Epoch 15: Train Loss = 0.4378, Val Loss = 0.3760\n",
      "Epoch 16: Train Loss = 0.4056, Val Loss = 0.4837\n",
      "Epoch 17: Train Loss = 0.4009, Val Loss = 0.1890\n",
      "Epoch 18: Train Loss = 0.2798, Val Loss = 0.1900\n",
      "Epoch 19: Train Loss = 0.2578, Val Loss = 0.1352\n",
      "Epoch 20: Train Loss = 0.2581, Val Loss = 0.2033\n",
      "Epoch 21: Train Loss = 0.2171, Val Loss = 0.1473\n",
      "Epoch 22: Train Loss = 0.2411, Val Loss = 0.3925\n",
      "Epoch 23: Train Loss = 0.2351, Val Loss = 0.1075\n",
      "Epoch 24: Train Loss = 0.1400, Val Loss = 0.0493\n",
      "Epoch 25: Train Loss = 0.1661, Val Loss = 0.0514\n",
      "\n",
      "LSTM Evaluation:\n",
      "Accuracy: 0.5407331975560081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.66      0.58      0.62       200\n",
      "        Folk       0.48      0.51      0.50       182\n",
      "         Pop       0.50      0.50      0.50       200\n",
      "         R&B       0.57      0.60      0.59       200\n",
      "        Rock       0.50      0.51      0.50       200\n",
      "\n",
      "    accuracy                           0.54       982\n",
      "   macro avg       0.54      0.54      0.54       982\n",
      "weighted avg       0.54      0.54      0.54       982\n",
      "\n",
      "\n",
      "Training RNN...\n",
      "Epoch 1: Train Loss = 1.5547, Val Loss = 1.4535\n",
      "Epoch 2: Train Loss = 1.4546, Val Loss = 1.4175\n",
      "Epoch 3: Train Loss = 1.4066, Val Loss = 1.3067\n",
      "Epoch 4: Train Loss = 1.3335, Val Loss = 1.2658\n",
      "Epoch 5: Train Loss = 1.2880, Val Loss = 1.2342\n",
      "Epoch 6: Train Loss = 1.2517, Val Loss = 1.2128\n",
      "Epoch 7: Train Loss = 1.1938, Val Loss = 1.1219\n",
      "Epoch 8: Train Loss = 1.1441, Val Loss = 1.1130\n",
      "Epoch 9: Train Loss = 1.0788, Val Loss = 1.0348\n",
      "Epoch 10: Train Loss = 1.0379, Val Loss = 0.9924\n",
      "Epoch 11: Train Loss = 0.9741, Val Loss = 0.9188\n",
      "Epoch 12: Train Loss = 0.9200, Val Loss = 0.8171\n",
      "Epoch 13: Train Loss = 0.8642, Val Loss = 0.7682\n",
      "Epoch 14: Train Loss = 0.8279, Val Loss = 0.7418\n",
      "Epoch 15: Train Loss = 0.7730, Val Loss = 0.6842\n",
      "Epoch 16: Train Loss = 0.7303, Val Loss = 0.6388\n",
      "Epoch 17: Train Loss = 0.7039, Val Loss = 0.5510\n",
      "Epoch 18: Train Loss = 0.6557, Val Loss = 0.5231\n",
      "Epoch 19: Train Loss = 0.6574, Val Loss = 0.5710\n",
      "Epoch 20: Train Loss = 0.8187, Val Loss = 0.5269\n",
      "Epoch 21: Train Loss = 0.6980, Val Loss = 0.5815\n",
      "Epoch 22: Train Loss = 0.5792, Val Loss = 0.4208\n",
      "Epoch 23: Train Loss = 0.5403, Val Loss = 0.4642\n",
      "Epoch 24: Train Loss = 0.5016, Val Loss = 0.3419\n",
      "Epoch 25: Train Loss = 0.4797, Val Loss = 0.3617\n",
      "\n",
      "RNN Evaluation:\n",
      "Accuracy: 0.5285132382892057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.70      0.58      0.64       200\n",
      "        Folk       0.49      0.53      0.51       182\n",
      "         Pop       0.41      0.50      0.45       200\n",
      "         R&B       0.58      0.57      0.58       200\n",
      "        Rock       0.51      0.46      0.48       200\n",
      "\n",
      "    accuracy                           0.53       982\n",
      "   macro avg       0.54      0.53      0.53       982\n",
      "weighted avg       0.54      0.53      0.53       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"kaggle_processed.csv\", on_bad_lines='skip', engine='python')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'kaggle_processed.csv' not found.\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna(subset=['Lyrics'])\n",
    "\n",
    "top_genres = df['Genre'].value_counts().nlargest(5).index\n",
    "df = df[df['Genre'].isin(top_genres)]\n",
    "print(f\"Top 5 genres: {top_genres.tolist()}\")\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Genre_encoded'] = le.fit_transform(df['Genre'])\n",
    "print(\"Encoded genres:\", le.classes_)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "remove_punct = str.maketrans('', '', punctuation)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(remove_punct)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "df['Clean_Lyrics'] = df['Lyrics'].apply(clean_text)\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Genre_encoded'], random_state=42)\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "bert_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "def get_token_level_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Embedding\"):\n",
    "        encoded = tokenizer(text, truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            output = bert_model(**encoded)\n",
    "        emb = output.last_hidden_state.squeeze(0)\n",
    "        embeddings.append(emb.cpu())\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "X_train = get_token_level_embeddings(train_df['Clean_Lyrics'])\n",
    "y_train = torch.tensor(train_df['Genre_encoded'].values, dtype=torch.long)\n",
    "\n",
    "X_test = get_token_level_embeddings(test_df['Clean_Lyrics'])\n",
    "y_test = torch.tensor(test_df['Genre_encoded'].values,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16)\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(input_dim, 128, kernel_size=4, padding=2)\n",
    "        self.conv3 = nn.Conv1d(input_dim, 128, kernel_size=5, padding=2)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(128 * 3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x1 = self.pool(self.relu(self.conv1(x))).squeeze(-1)\n",
    "        x2 = self.pool(self.relu(self.conv2(x))).squeeze(-1)\n",
    "        x3 = self.pool(self.relu(self.conv3(x))).squeeze(-1)\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True, dropout=0.4)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        out = output[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=2, batch_first=True, nonlinearity='relu', dropout=0.4)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        out = output[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=25, patience=5):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    " \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), f\"best_{model.__class__.__name__}.pt\")\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"best_{model.__class__.__name__}.pt\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, loader, y_true):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb)\n",
    "            preds.append(torch.argmax(pred, dim=1).cpu())\n",
    "    y_pred = torch.cat(preds)\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['Genre_encoded'], random_state=42)\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "X_val = get_token_level_embeddings(val_df['Clean_Lyrics'])\n",
    "y_val = torch.tensor(val_df['Genre_encoded'].values, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16)\n",
    "\n",
    "\n",
    "print(\"\\nTraining CNN...\")\n",
    "cnn = TextCNN(input_dim=X_train.shape[2], num_classes=len(le.classes_))\n",
    "cnn_model = train_model(cnn, train_loader, val_loader)\n",
    "print(\"\\nCNN Evaluation:\")\n",
    "evaluate(cnn_model, test_loader, y_test)\n",
    "\n",
    "print(\"\\nTraining LSTM...\")\n",
    "lstm = TextLSTM(input_dim=X_train.shape[2], hidden_dim=128, num_classes=len(le.classes_))\n",
    "lstm_model = train_model(lstm, train_loader, val_loader)\n",
    "print(\"\\nLSTM Evaluation:\")\n",
    "evaluate(lstm_model, test_loader, y_test)\n",
    "\n",
    "print(\"\\nTraining RNN...\")\n",
    "rnn = TextRNN(input_dim=X_train.shape[2], hidden_dim=128, num_classes=len(le.classes_))\n",
    "rnn_model = train_model(rnn, train_loader, val_loader)\n",
    "print(\"\\nRNN Evaluation:\")\n",
    "evaluate(rnn_model, test_loader, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb7cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(cnn_model, lstm_model, loader, y_true):\n",
    "    cnn_model.eval()\n",
    "    lstm_model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "\n",
    "            prob_cnn = torch.softmax(cnn_model(xb), dim=1)\n",
    "            prob_lstm = torch.softmax(lstm_model(xb), dim=1)\n",
    "\n",
    "\n",
    "            ensemble_prob = (prob_cnn + prob_lstm) / 2\n",
    "            preds = torch.argmax(ensemble_prob, dim=1).cpu()\n",
    "            all_preds.append(preds)\n",
    "\n",
    "    y_pred = torch.cat(all_preds)\n",
    "    print(\"Ensemble Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9592d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating CNN + LSTM Ensemble:\n",
      "Ensemble Accuracy: 0.5936863543788188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.69      0.65      0.67       200\n",
      "        Folk       0.54      0.64      0.59       182\n",
      "         Pop       0.57      0.55      0.56       200\n",
      "         R&B       0.64      0.62      0.63       200\n",
      "        Rock       0.54      0.52      0.53       200\n",
      "\n",
      "    accuracy                           0.59       982\n",
      "   macro avg       0.60      0.59      0.59       982\n",
      "weighted avg       0.60      0.59      0.59       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Evaluating CNN + LSTM Ensemble:\")\n",
    "evaluate_ensemble(cnn_model, lstm_model, test_loader, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9b1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_weighted_ensemble(cnn_model, lstm_model, loader, y_true):\n",
    "    cnn_model.eval()\n",
    "    lstm_model.eval()\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_w = None\n",
    "\n",
    "    for w in [0.1 * i for i in range(1, 10)]:  \n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in loader:\n",
    "                xb = xb.to(device)\n",
    "\n",
    "                cnn_probs = torch.softmax(cnn_model(xb), dim=1)\n",
    "                lstm_probs = torch.softmax(lstm_model(xb), dim=1)\n",
    "\n",
    "                ensemble_prob = w * cnn_probs + (1 - w) * lstm_probs\n",
    "                pred = torch.argmax(ensemble_prob, dim=1).cpu()\n",
    "                preds.append(pred)\n",
    "\n",
    "        y_pred = torch.cat(preds)\n",
    "        f1 = accuracy_score(y_true, y_pred)\n",
    "        print(f\"w = {w:.1f} → Accuracy = {f1:.4f}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_w = w\n",
    "\n",
    "    print(f\"\\n Best Weight: {best_w:.1f} | Accuracy: {best_f1:.4f}\")\n",
    "    return best_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db189f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 0.1 → Accuracy = 0.5428\n",
      "w = 0.2 → Accuracy = 0.5438\n",
      "w = 0.3 → Accuracy = 0.5499\n",
      "w = 0.4 → Accuracy = 0.5662\n",
      "w = 0.5 → Accuracy = 0.5937\n",
      "w = 0.6 → Accuracy = 0.6171\n",
      "w = 0.7 → Accuracy = 0.6141\n",
      "w = 0.8 → Accuracy = 0.6141\n",
      "w = 0.9 → Accuracy = 0.6090\n",
      "\n",
      " Best Weight: 0.6 | Accuracy: 0.6171\n"
     ]
    }
   ],
   "source": [
    "best_w = evaluate_weighted_ensemble(cnn_model, lstm_model, test_loader, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6dcf970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {k: v.squeeze() for k, v in encoding.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b541ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_probs(texts, model, tokenizer):\n",
    "    dataset = SimpleTextDataset(texts, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    return torch.cat(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c1bd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 99/99 [00:42<00:00,  2.33it/s]\n",
      "Batches: 100%|██████████| 31/31 [00:13<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT + MLP...\n",
      "\n",
      " Classification Report (BERT + MLP):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.61      0.57      0.59       200\n",
      "        Folk       0.50      0.47      0.48       182\n",
      "         Pop       0.52      0.54      0.53       200\n",
      "         R&B       0.61      0.66      0.63       200\n",
      "        Rock       0.45      0.46      0.46       200\n",
      "\n",
      "    accuracy                           0.54       982\n",
      "   macro avg       0.54      0.54      0.54       982\n",
      "weighted avg       0.54      0.54      0.54       982\n",
      "\n",
      "Accuracy: 0.539714867617108\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "\n",
    "\n",
    "print(\"Generating BERT embeddings...\")\n",
    "bert_embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "X_train_bert = bert_embedder.encode(train_df['Clean_Lyrics'].tolist(), show_progress_bar=True, batch_size=32)\n",
    "X_test_bert = bert_embedder.encode(test_df['Clean_Lyrics'].tolist(), show_progress_bar=True, batch_size=32)\n",
    "\n",
    "y_train_bert = train_df['Genre_encoded'].values\n",
    "y_test_bert = test_df['Genre_encoded'].values\n",
    "\n",
    "print(\"Training BERT + MLP...\")\n",
    "mlp_bert = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=500, random_state=42)\n",
    "mlp_bert.fit(X_train_bert, y_train_bert)\n",
    "\n",
    "y_pred_bert = mlp_bert.predict(X_test_bert)\n",
    "bert_probs = torch.tensor(mlp_bert.predict_proba(X_test_bert))\n",
    "\n",
    "print(\"\\n Classification Report (BERT + MLP):\")\n",
    "print(classification_report(y_test_bert, y_pred_bert, target_names=le.classes_))\n",
    "print(\"Accuracy:\", accuracy_score(y_test_bert, y_pred_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5954d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble_three_way(cnn_model, lstm_model, bert_probs, loader, y_true, weights=(0.8, 0.4, 0.8)):\n",
    "    cnn_model.eval()\n",
    "    lstm_model.eval()\n",
    "    w_cnn, w_lstm, w_bert = weights\n",
    "\n",
    "    preds = []\n",
    "    i = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "\n",
    "            cnn_probs = torch.softmax(cnn_model(xb), dim=1)\n",
    "            lstm_probs = torch.softmax(lstm_model(xb), dim=1)\n",
    "\n",
    "            batch_size = xb.size(0)\n",
    "            bert_batch_probs = bert_probs[i:i+batch_size]\n",
    "            i += batch_size\n",
    "\n",
    "            final_probs = w_cnn * cnn_probs.cpu() + w_lstm * lstm_probs.cpu() + w_bert * bert_batch_probs\n",
    "            preds.append(torch.argmax(final_probs, dim=1))\n",
    "\n",
    "    y_pred = torch.cat(preds)\n",
    "    print(\"Ensemble Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802098a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating CNN + LSTM + BERT Ensemble:\n",
      "Ensemble Accuracy: 0.6262729124236253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.72      0.66      0.68       200\n",
      "        Folk       0.56      0.62      0.59       182\n",
      "         Pop       0.62      0.59      0.61       200\n",
      "         R&B       0.66      0.68      0.67       200\n",
      "        Rock       0.59      0.58      0.59       200\n",
      "\n",
      "    accuracy                           0.63       982\n",
      "   macro avg       0.63      0.63      0.63       982\n",
      "weighted avg       0.63      0.63      0.63       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Evaluating CNN + LSTM + BERT Ensemble:\")\n",
    "evaluate_ensemble_three_way(cnn_model, lstm_model, bert_probs, test_loader, y_test, weights=(0.8, 0.7, 0.8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
