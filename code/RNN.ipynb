{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a39f01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saksh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 genres: ['Pop', 'Rock', 'Country', 'R&B', 'Folk']\n",
      "Encoded genres: ['Country' 'Folk' 'Pop' 'R&B' 'Rock']\n",
      "Preprocessing lyrics...\n",
      "Train size: 3925, Test size: 982\n",
      "Encoding train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 3925/3925 [11:51<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 982/982 [02:58<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3140, Val size: 785, Test size: 982\n",
      "Encoding validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 785/785 [02:21<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN...\n",
      "Epoch 1: Train Loss = 1.4979, Val Loss = 1.2231\n",
      "Epoch 2: Train Loss = 1.2715, Val Loss = 1.0739\n",
      "Epoch 3: Train Loss = 1.1406, Val Loss = 0.9036\n",
      "Epoch 4: Train Loss = 1.0097, Val Loss = 0.8017\n",
      "Epoch 5: Train Loss = 0.8818, Val Loss = 0.6456\n",
      "Epoch 6: Train Loss = 0.7562, Val Loss = 0.5265\n",
      "Epoch 7: Train Loss = 0.6551, Val Loss = 0.4066\n",
      "Epoch 8: Train Loss = 0.5451, Val Loss = 0.3391\n",
      "Epoch 9: Train Loss = 0.4815, Val Loss = 0.2604\n",
      "Epoch 10: Train Loss = 0.4008, Val Loss = 0.1856\n",
      "Epoch 11: Train Loss = 0.3601, Val Loss = 0.1382\n",
      "Epoch 12: Train Loss = 0.3095, Val Loss = 0.1319\n",
      "Epoch 13: Train Loss = 0.2644, Val Loss = 0.0848\n",
      "Epoch 14: Train Loss = 0.2429, Val Loss = 0.0743\n",
      "Epoch 15: Train Loss = 0.2170, Val Loss = 0.1212\n",
      "Epoch 16: Train Loss = 0.2267, Val Loss = 0.0472\n",
      "Epoch 17: Train Loss = 0.1909, Val Loss = 0.0348\n",
      "Epoch 18: Train Loss = 0.1713, Val Loss = 0.0226\n",
      "Epoch 19: Train Loss = 0.1730, Val Loss = 0.0353\n",
      "Epoch 20: Train Loss = 0.1699, Val Loss = 0.0580\n",
      "Epoch 21: Train Loss = 0.1794, Val Loss = 0.0406\n",
      "Epoch 22: Train Loss = 0.1602, Val Loss = 0.0425\n",
      "Epoch 23: Train Loss = 0.1565, Val Loss = 0.0255\n",
      "Early stopping triggered!\n",
      "\n",
      "CNN Evaluation:\n",
      "Accuracy: 0.5987780040733197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.66      0.66      0.66       200\n",
      "        Folk       0.54      0.55      0.54       182\n",
      "         Pop       0.58      0.58      0.58       200\n",
      "         R&B       0.66      0.67      0.66       200\n",
      "        Rock       0.55      0.54      0.54       200\n",
      "\n",
      "    accuracy                           0.60       982\n",
      "   macro avg       0.60      0.60      0.60       982\n",
      "weighted avg       0.60      0.60      0.60       982\n",
      "\n",
      "\n",
      "Training LSTM...\n",
      "Epoch 1: Train Loss = 1.5207, Val Loss = 1.4223\n",
      "Epoch 2: Train Loss = 1.4003, Val Loss = 1.3325\n",
      "Epoch 3: Train Loss = 1.3396, Val Loss = 1.2734\n",
      "Epoch 4: Train Loss = 1.2843, Val Loss = 1.2697\n",
      "Epoch 5: Train Loss = 1.2273, Val Loss = 1.1321\n",
      "Epoch 6: Train Loss = 1.1688, Val Loss = 1.0931\n",
      "Epoch 7: Train Loss = 1.1087, Val Loss = 1.0150\n",
      "Epoch 8: Train Loss = 1.0459, Val Loss = 0.9010\n",
      "Epoch 9: Train Loss = 0.9782, Val Loss = 0.8644\n",
      "Epoch 10: Train Loss = 0.8866, Val Loss = 0.7583\n",
      "Epoch 11: Train Loss = 0.7875, Val Loss = 0.6873\n",
      "Epoch 12: Train Loss = 0.6866, Val Loss = 0.6257\n",
      "Epoch 13: Train Loss = 0.6175, Val Loss = 0.4934\n",
      "Epoch 14: Train Loss = 0.5269, Val Loss = 0.4797\n",
      "Epoch 15: Train Loss = 0.4737, Val Loss = 0.3805\n",
      "Epoch 16: Train Loss = 0.4378, Val Loss = 0.2952\n",
      "Epoch 17: Train Loss = 0.3865, Val Loss = 0.2715\n",
      "Epoch 18: Train Loss = 0.3232, Val Loss = 0.2181\n",
      "Epoch 19: Train Loss = 0.2944, Val Loss = 0.1661\n",
      "Epoch 20: Train Loss = 0.2704, Val Loss = 0.2040\n",
      "Epoch 21: Train Loss = 0.2456, Val Loss = 0.1115\n",
      "Epoch 22: Train Loss = 0.2213, Val Loss = 0.1511\n",
      "Epoch 23: Train Loss = 0.1652, Val Loss = 0.1155\n",
      "Epoch 24: Train Loss = 0.1846, Val Loss = 0.0795\n",
      "Epoch 25: Train Loss = 0.1558, Val Loss = 0.1230\n",
      "\n",
      "LSTM Evaluation:\n",
      "Accuracy: 0.5417515274949084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.67      0.57      0.62       200\n",
      "        Folk       0.53      0.43      0.48       182\n",
      "         Pop       0.51      0.55      0.53       200\n",
      "         R&B       0.56      0.58      0.57       200\n",
      "        Rock       0.47      0.56      0.51       200\n",
      "\n",
      "    accuracy                           0.54       982\n",
      "   macro avg       0.55      0.54      0.54       982\n",
      "weighted avg       0.55      0.54      0.54       982\n",
      "\n",
      "\n",
      "Training RNN...\n",
      "Epoch 1: Train Loss = 1.5674, Val Loss = 1.4817\n",
      "Epoch 2: Train Loss = 1.4579, Val Loss = 1.3796\n",
      "Epoch 3: Train Loss = 1.3833, Val Loss = 1.3368\n",
      "Epoch 4: Train Loss = 1.3326, Val Loss = 1.2809\n",
      "Epoch 5: Train Loss = 1.2838, Val Loss = 1.2268\n",
      "Epoch 6: Train Loss = 1.2470, Val Loss = 1.1306\n",
      "Epoch 7: Train Loss = 1.1910, Val Loss = 1.0734\n",
      "Epoch 8: Train Loss = 1.1354, Val Loss = 1.0678\n",
      "Epoch 9: Train Loss = 1326170588336.0522, Val Loss = 1.2301\n",
      "Epoch 10: Train Loss = 1.2352, Val Loss = 1.2374\n",
      "Epoch 11: Train Loss = 1.1719, Val Loss = 1.0622\n",
      "Epoch 12: Train Loss = 1.1177, Val Loss = 1.0324\n",
      "Epoch 13: Train Loss = 1.0613, Val Loss = 1.0092\n",
      "Epoch 14: Train Loss = 1.0501, Val Loss = 0.9458\n",
      "Epoch 15: Train Loss = 0.9795, Val Loss = 0.8634\n",
      "Epoch 16: Train Loss = 0.9405, Val Loss = 0.8162\n",
      "Epoch 17: Train Loss = 0.8934, Val Loss = 0.7970\n",
      "Epoch 18: Train Loss = 0.8802, Val Loss = 0.7794\n",
      "Epoch 19: Train Loss = 0.8384, Val Loss = 0.8087\n",
      "Epoch 20: Train Loss = 0.8464, Val Loss = 0.7220\n",
      "Epoch 21: Train Loss = 0.8024, Val Loss = 0.6951\n",
      "Epoch 22: Train Loss = 0.7635, Val Loss = 0.6782\n",
      "Epoch 23: Train Loss = 0.7567, Val Loss = 0.6605\n",
      "Epoch 24: Train Loss = 0.7311, Val Loss = 0.6387\n",
      "Epoch 25: Train Loss = 13.4422, Val Loss = 1.2864\n",
      "\n",
      "RNN Evaluation:\n",
      "Accuracy: 0.4908350305498982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Country       0.55      0.66      0.60       200\n",
      "        Folk       0.53      0.43      0.48       182\n",
      "         Pop       0.44      0.30      0.36       200\n",
      "         R&B       0.51      0.62      0.56       200\n",
      "        Rock       0.41      0.43      0.42       200\n",
      "\n",
      "    accuracy                           0.49       982\n",
      "   macro avg       0.49      0.49      0.48       982\n",
      "weighted avg       0.49      0.49      0.48       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"kaggle_processed.csv\", on_bad_lines='skip', engine='python')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'kaggle_processed.csv' not found.\")\n",
    "    exit(1)\n",
    "\n",
    "# Clean column names and drop NA\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.dropna(subset=['Lyrics'])\n",
    "\n",
    "# Keep top 5 genres\n",
    "top_genres = df['Genre'].value_counts().nlargest(5).index\n",
    "df = df[df['Genre'].isin(top_genres)]\n",
    "print(f\"Top 5 genres: {top_genres.tolist()}\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['Genre_encoded'] = le.fit_transform(df['Genre'])\n",
    "print(\"Encoded genres:\", le.classes_)\n",
    "\n",
    "# Preprocessing with lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "remove_punct = str.maketrans('', '', punctuation)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(remove_punct)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Preprocessing lyrics...\")\n",
    "df['Clean_Lyrics'] = df['Lyrics'].apply(clean_text)\n",
    "\n",
    "# Split into train and test (80-20)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Genre_encoded'], random_state=42)\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# DistilBERT Embeddings\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "bert_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "def get_token_level_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Embedding\"):\n",
    "        encoded = tokenizer(text, truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            output = bert_model(**encoded)\n",
    "        emb = output.last_hidden_state.squeeze(0)\n",
    "        embeddings.append(emb.cpu())\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "print(\"Encoding train set...\")\n",
    "X_train = get_token_level_embeddings(train_df['Clean_Lyrics'])\n",
    "y_train = torch.tensor(train_df['Genre_encoded'].values, dtype=torch.long)\n",
    "\n",
    "print(\"Encoding test set...\")\n",
    "X_test = get_token_level_embeddings(test_df['Clean_Lyrics'])\n",
    "y_test = torch.tensor(test_df['Genre_encoded'].values,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16)\n",
    "\n",
    "# RNN Model\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=2, batch_first=True, nonlinearity='relu', dropout=0.4)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        out = output[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, epochs=25, patience=5):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), f\"best_{model.__class__.__name__}.pt\")\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f\"best_{model.__class__.__name__}.pt\"))\n",
    "    return model\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader, y_true):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb)\n",
    "            preds.append(torch.argmax(pred, dim=1).cpu())\n",
    "    y_pred = torch.cat(preds)\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Create validation set\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['Genre_encoded'], random_state=42)\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "print(\"Encoding validation set...\")\n",
    "X_val = get_token_level_embeddings(val_df['Clean_Lyrics'])\n",
    "y_val = torch.tensor(val_df['Genre_encoded'].values, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16)\n",
    "\n",
    "print(\"\\nTraining RNN...\")\n",
    "rnn = TextRNN(input_dim=X_train.shape[2], hidden_dim=128, num_classes=len(le.classes_))\n",
    "rnn_model = train_model(rnn, train_loader, val_loader)\n",
    "print(\"\\nRNN Evaluation:\")\n",
    "evaluate(rnn_model, test_loader, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
